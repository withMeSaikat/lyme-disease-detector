{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries\n> `OpenCV` is not used.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage import io\nfrom skimage.transform import rescale, resize, downscale_local_mean\n#import cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image as Image\nfrom tensorflow.keras.applications import VGG19\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras.callbacks import ReduceLROnPlateau\nimport warnings\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:16:38.293519Z","iopub.execute_input":"2022-01-01T10:16:38.293795Z","iopub.status.idle":"2022-01-01T10:16:38.300895Z","shell.execute_reply.started":"2022-01-01T10:16:38.293767Z","shell.execute_reply":"2022-01-01T10:16:38.299951Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"root = \"../input/lyme-disease-rashes/\"\nimg = io.imread(root + \"RashData/Lyme_Positive_By_Diease/EMRash/erythema migrans42.jpg\")\nplt.imshow(img)\nplt.title(\"Raw Image\")","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:04:19.803897Z","iopub.execute_input":"2022-01-01T10:04:19.804215Z","iopub.status.idle":"2022-01-01T10:04:20.119316Z","shell.execute_reply.started":"2022-01-01T10:04:19.804180Z","shell.execute_reply":"2022-01-01T10:04:20.118597Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"HEIGHT, WIDTH = 300, 300\nresized_img = resize(img, (HEIGHT, WIDTH))\nplt.imshow(resized_img)\nplt.title(\"Resized Image\")","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:04:21.598968Z","iopub.execute_input":"2022-01-01T10:04:21.599697Z","iopub.status.idle":"2022-01-01T10:04:21.908863Z","shell.execute_reply.started":"2022-01-01T10:04:21.599658Z","shell.execute_reply":"2022-01-01T10:04:21.907693Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Importing the data\n1. Loading the data from root directory\n2. Each image is resized into `200 * 200` dimension\n3. The imported image is converted into an numpy array\n4. Each data is normalized","metadata":{}},{"cell_type":"code","source":"# Creating the dataset for processing -- Using SKIMAGE\npath_root = root + \"RashData/Train/Train_2_Cases\"\ndata = []\ncategories = [\"Negative\", \"Positive\"]\nflag = True\nfor category in categories:\n    path = os.path.join(path_root, \"Lyme_\" + category)\n    label = categories.index(category)\n    \n    for img in os.listdir(path):\n        try:\n            img_path = os.path.join(path, img)\n            dis_img = io.imread(img_path)\n            if flag:\n                print(type(dis_img))\n                flag = False\n    \n            image = resize(dis_img, (HEIGHT, WIDTH))\n#             image = np.array(dis_img).flatten()\n            data.append([image, label])\n        except Exception as e:\n            print(\"Could not add image.\")","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spliting the data array into -\n* Image Feature Vector(image_ds)\n* Corresponding Labels(labels)","metadata":{}},{"cell_type":"code","source":"\nimage_ds = np.array([obj[0][:,:,0] for obj in data])\nlabels = np.array([obj[1] for obj in data])\nprint(image_ds.shape)\n#image_ds = image_ds.reshape((356, 200, 200, 1))\nlen(labels[labels == 1])","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:04:51.151757Z","iopub.execute_input":"2022-01-01T10:04:51.152340Z","iopub.status.idle":"2022-01-01T10:04:51.309607Z","shell.execute_reply.started":"2022-01-01T10:04:51.152283Z","shell.execute_reply":"2022-01-01T10:04:51.308878Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Creating Train and Test Data\n> Using 20 percent of data as the test data.","metadata":{}},{"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(image_ds, labels, test_size=0.20, shuffle=True)\nprint(\"Size of training data:\", train_x.shape)\nprint(\"Size of test data:\", test_x.shape)\nlen(train_y[train_y==1])","metadata":{"execution":{"iopub.status.busy":"2021-12-29T15:07:55.651236Z","iopub.execute_input":"2021-12-29T15:07:55.651852Z","iopub.status.idle":"2021-12-29T15:07:55.739851Z","shell.execute_reply.started":"2021-12-29T15:07:55.65179Z","shell.execute_reply":"2021-12-29T15:07:55.739158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Support Vector Machine for Trainning:\n1. The training and testing data is flattened\n2. Model is defined with a `Polynomial` kernel\n3. Model is trained \n4. Confusion Matrix and Classification Report is observed","metadata":{}},{"cell_type":"code","source":"train_x_flat = train_x.reshape((train_x.shape[0], HEIGHT * WIDTH))\ntest_x_flat = test_x.reshape((test_x.shape[0], HEIGHT * WIDTH))\nprint(train_x_flat.shape)\nmodel_svm = SVC(kernel='poly', gamma='auto')\nmodel_svm.fit(train_x_flat, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T15:07:58.499849Z","iopub.execute_input":"2021-12-29T15:07:58.500125Z","iopub.status.idle":"2021-12-29T15:08:07.94711Z","shell.execute_reply.started":"2021-12-29T15:07:58.500092Z","shell.execute_reply":"2021-12-29T15:08:07.946279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_y = model_svm.predict(test_x_flat)\nprint(confusion_matrix(test_y, pred_y))\nprint(classification_report(test_y, pred_y))","metadata":{"execution":{"iopub.status.busy":"2021-12-29T15:08:10.661238Z","iopub.execute_input":"2021-12-29T15:08:10.661497Z","iopub.status.idle":"2021-12-29T15:08:12.92728Z","shell.execute_reply.started":"2021-12-29T15:08:10.661468Z","shell.execute_reply":"2021-12-29T15:08:12.925167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Deep Feed Forward Network to train\n+ Using a fully connected network to train \n+ The network consists of two hidden layer of size `20000 * 1`\n+ Finally using a output layer with `sigmoid` activation as it is binary classification\n> Assuming the input images are of dimension `100 * 100`.","metadata":{}},{"cell_type":"code","source":"# model_dense = keras.models.Sequential([\n#     keras.Input(shape=(100 * 100,)),\n#     keras.layers.Dense(2 * 100 * 100, activation='relu'),\n#     keras.layers.Dense(2 * 100 * 100, activation='relu'),\n#     keras.layers.Dense(1)\n# ])\n# model_dense.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# model_dense.fit(train_x_flat, train_y, batch_size=32, epochs=20,steps_per_epoch=4)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T15:05:37.099329Z","iopub.execute_input":"2021-12-29T15:05:37.099571Z","iopub.status.idle":"2021-12-29T15:05:37.103445Z","shell.execute_reply.started":"2021-12-29T15:05:37.099536Z","shell.execute_reply":"2021-12-29T15:05:37.10277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# report = model_dense.evaluate(test_x_flat, test_y)\n# print(report)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T15:05:37.104942Z","iopub.execute_input":"2021-12-29T15:05:37.105402Z","iopub.status.idle":"2021-12-29T15:05:37.113287Z","shell.execute_reply.started":"2021-12-29T15:05:37.105367Z","shell.execute_reply":"2021-12-29T15:05:37.112523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using CNN to train\nThis is executed in following steps:\n1. First we rescale the training and validation data in range `[0, 1]`\n2. Then we define 5 convolution layers with MaxPool layers and `relu` activation\n3. Last output layer is of size 1 and using `sigmoid` activation function","metadata":{}},{"cell_type":"code","source":"train = ImageDataGenerator(rescale=1/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\nvalidation = ImageDataGenerator(rescale=1/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n\ntrain_ds = train.flow_from_directory(root + \"RashData/Train/Train_2_Cases\", target_size=(HEIGHT, WIDTH),\n                                    batch_size=10, class_mode='binary')\nval_ds = validation.flow_from_directory(root + \"RashData/Validation/Validation_2_Cases\", target_size=(HEIGHT, WIDTH),\n                                       batch_size=10, class_mode='binary')\nval_ds.class_indices\nfilter_size = (2, 2)\nepochs = 25","metadata":{"execution":{"iopub.status.busy":"2022-01-01T11:07:07.952291Z","iopub.execute_input":"2022-01-01T11:07:07.952565Z","iopub.status.idle":"2022-01-01T11:07:08.166530Z","shell.execute_reply.started":"2022-01-01T11:07:07.952536Z","shell.execute_reply":"2022-01-01T11:07:08.165806Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"model_conv_2d = keras.models.Sequential([\n    keras.layers.Conv2D(32, filter_size, activation='relu', input_shape=(HEIGHT, WIDTH, 3), kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(2, 2),\n    keras.layers.Conv2D(64, filter_size, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(2, 2),\n    keras.layers.Conv2D(128, filter_size, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(2, 2),\n    keras.layers.Conv2D(256, filter_size, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(2, 2),\n    keras.layers.Conv2D(512, filter_size, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(2, 2),\n    keras.layers.Flatten(),\n    keras.layers.Dense(1024, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n    keras.layers.Dense(1, activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2022-01-01T11:07:09.082194Z","iopub.execute_input":"2022-01-01T11:07:09.082454Z","iopub.status.idle":"2022-01-01T11:07:09.202043Z","shell.execute_reply.started":"2022-01-01T11:07:09.082425Z","shell.execute_reply":"2022-01-01T11:07:09.201384Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"model_conv_2d.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory_conv_2d = model_conv_2d.fit(train_ds, steps_per_epoch=3, epochs=epochs, validation_data=val_ds)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T11:07:13.292133Z","iopub.execute_input":"2022-01-01T11:07:13.292689Z","iopub.status.idle":"2022-01-01T11:09:07.924562Z","shell.execute_reply.started":"2022-01-01T11:07:13.292650Z","shell.execute_reply":"2022-01-01T11:09:07.923781Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\n\nepochs_range = range(1, epochs + 1)\ntrain_loss = history_conv_2d.history['loss']\nval_loss = history_conv_2d.history['val_loss']\n# train_auc = history.history['auc']\n# val_auc = history.history['val_auc']\n\n# plt.subplot(1, 2, 1)\nplt.plot(epochs_range, train_loss, label=\"Training Loss\")\nplt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation Loss\")\nplt.legend()\n\n# plt.subplot(1, 2, 2)\n# plt.plot(epochs_range, train_auc, label=\"Training AUC\", color='b')\n# plt.plot(epochs_range, val_auc, label=\"Validation AUC\", color='r')\n\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"AUC\")\n# plt.title(\"Training and Validation AUC\")\n# plt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T11:09:34.852079Z","iopub.execute_input":"2022-01-01T11:09:34.852346Z","iopub.status.idle":"2022-01-01T11:09:35.089713Z","shell.execute_reply.started":"2022-01-01T11:09:34.852319Z","shell.execute_reply":"2022-01-01T11:09:35.089010Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"## Using different state of the art models\n+ VGG19\n+ ResNet","metadata":{}},{"cell_type":"code","source":"DIMS_VGG = (224, 224, 3)\ntrain_vgg = train.flow_from_directory(root + \"RashData/Train/Train_2_Cases\", target_size=(DIMS_VGG[0], DIMS_VGG[1]),\n                                    batch_size=10, class_mode='binary')\nval_vgg = validation.flow_from_directory(root + \"RashData/Validation/Validation_2_Cases\", \n                                         target_size=(DIMS_VGG[0], DIMS_VGG[1]), batch_size=10, class_mode='binary')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:05:10.053667Z","iopub.execute_input":"2022-01-01T10:05:10.053924Z","iopub.status.idle":"2022-01-01T10:05:10.263941Z","shell.execute_reply.started":"2022-01-01T10:05:10.053895Z","shell.execute_reply":"2022-01-01T10:05:10.263191Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nSTEP_SIZE_TRAIN = train_vgg.n // train_vgg.batch_size\nSTEP_SIZE_VAL = val_vgg.n // val_vgg.batch_size\nEPOCH_VGG = 30","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:06:02.333745Z","iopub.execute_input":"2022-01-01T10:06:02.334542Z","iopub.status.idle":"2022-01-01T10:06:02.340613Z","shell.execute_reply.started":"2022-01-01T10:06:02.334493Z","shell.execute_reply":"2022-01-01T10:06:02.339359Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"vgg = VGG19(input_shape=DIMS_VGG, weights='imagenet', include_top=False)\n\nfor layer in vgg.layers:\n    layer.trainable = False\n\nx = keras.layers.Flatten()(vgg.output)\nhidd1 = keras.layers.Dense(1024, activation='relu')(x)\npred = keras.layers.Dense(1, activation='sigmoid')(hidd1)\n\nmodel_vgg = keras.Model(inputs=vgg.input, outputs=pred)\n\nmodel_vgg.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel_vgg.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:06:03.503401Z","iopub.execute_input":"2022-01-01T10:06:03.503657Z","iopub.status.idle":"2022-01-01T10:06:04.039879Z","shell.execute_reply.started":"2022-01-01T10:06:03.503630Z","shell.execute_reply":"2022-01-01T10:06:04.039083Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5,\n                               min_lr=0.5e-6)\ncheckpoint = ModelCheckpoint(filepath='./Models/model_vgg.h5', verbose=1, save_best_only=True)\n\ncallbacks = [checkpoint, lr_reducer]","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:06:04.563596Z","iopub.execute_input":"2022-01-01T10:06:04.563829Z","iopub.status.idle":"2022-01-01T10:06:04.569015Z","shell.execute_reply.started":"2022-01-01T10:06:04.563801Z","shell.execute_reply":"2022-01-01T10:06:04.568361Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"start = datetime.now()\nhistory_vgg = model_vgg.fit_generator(train_vgg, \n                    steps_per_epoch=STEP_SIZE_TRAIN, \n                    epochs = EPOCH_VGG, verbose=5, \n                    validation_data = val_vgg, \n                    validation_steps = STEP_SIZE_VAL,\n                    callbacks=[checkpoint])\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:06:07.933459Z","iopub.execute_input":"2022-01-01T10:06:07.933712Z","iopub.status.idle":"2022-01-01T10:11:07.914587Z","shell.execute_reply.started":"2022-01-01T10:06:07.933684Z","shell.execute_reply":"2022-01-01T10:11:07.913822Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"score_val = model_vgg.evaluate(val_vgg)\nscore_train = model_vgg.evaluate(train_vgg)\nprint('Test Loss:', score_val[0])\nprint('Test accuracy:', score_val[1])\nprint('Train Loss:', score_train[0])\nprint('Train accuracy:', score_train[1])\nplt.plot(history_vgg.history['accuracy'])\nplt.plot(history_vgg.history['val_accuracy'])\nplt.plot(history_vgg.history['loss'])\nplt.plot(history_vgg.history['val_loss'])\nplt.title('model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Accuracy','Validation Accuracy','loss','Validation Loss'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:11:11.788484Z","iopub.execute_input":"2022-01-01T10:11:11.788732Z","iopub.status.idle":"2022-01-01T10:11:21.133790Z","shell.execute_reply.started":"2022-01-01T10:11:11.788705Z","shell.execute_reply":"2022-01-01T10:11:21.133105Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing the intermediate activation layers\n> Visualizing the ouputs in each convolution layer in the neural network gives us interesting insights about the workings of the trained filters in the convolution.","metadata":{}},{"cell_type":"code","source":"p = root + \"RashData/Lyme_Positive_By_Diease/EMRash/erythema migrans42.jpg\"\ndef plot_interm_act(classifier, no_of_layers, img_path, img_size, model_name=None):\n    if model_name != None:\n        print(\"Plotting first\", no_of_layers, \"of the\", model_name, \"model.\")\n        \n    # Selecting first 'no_of_layers' no of layers from the classifier\n    output_layers = [layer.output for layer in classifier.layers[:no_of_layers + 1]]\n    \n    # Defining the input layer\n    input_layer = classifier.input\n    # Instanciating the model\n    model =  keras.Model(inputs=input_layer, outputs=output_layers)\n    \n    # Loading the image from the given image path\n    ## Transforming the loaded image to array\n    ### Expanding the dimensions -- Because Keras needs one extra dimension\n    img = Image.load_img(img_path, target_size=img_size)\n    img_tensor = Image.img_to_array(img)\n    img_tensor = np.expand_dims(img_tensor, axis=0)\n    \n    \n    # Plotting the three different color channels of the input image\n    input_grid = np.zeros((img_size[0] * 1, img_size[0] * 3))\n    input_grid[0:img_size[1], 0 * img_size[1]: 1 * img_size[1]] = img_tensor[0,:,:,0]\n    input_grid[0:img_size[1], 1 * img_size[1]: 2 * img_size[1]] = img_tensor[0,:,:,1]\n    input_grid[0:img_size[1], 2 * img_size[1]: 3 * img_size[1]] = img_tensor[0,:,:,2]    \n    plt.title('input_layer')\n    plt.imshow(input_grid)\n\n    # Normalizing the values of the image tensor\n    img_tensor /= 255.\n    print(\"Shape of current resized image: \", img_tensor.shape)\n    \n    # Feeding the image tensor to the model and getting saving outputs of each layer in 'activations'\n    activations = model.predict(img_tensor)\n    \n    # Saving the corresponding layer names for plotting\n    layer_names = list([layer.name for layer in classifier.layers[:no_of_layers+1]])\n    \n    # Iterating through all layers\n    for layer_name, activation in zip(layer_names, activations):\n        plots_per_row = 16 # No of channels to be plotted in each row\n        \n        no_of_channels = activation.shape[-1] # No of channels in current output layer\n        \n        # If current no of channels is less than 'plots_per_row'\n        if no_of_channels < plots_per_row:\n            plots_per_row = no_of_channels\n        \n        # Size of the current output -- no_of_cols == no_of_row in output\n        size = activation.shape[1]\n        \n        # No of row's required\n        no_row = no_of_channels // plots_per_row\n        \n        # Defining a grid in which all channels of current layer is concatenated\n        grid = np.zeros((size * no_row, plots_per_row * size))\n    \n        for i in range(no_row):\n            for j in range(plots_per_row):\n                channel_img = activation[0,:,:,i * plots_per_row + j]\n                \n                # Normalizing the image\n                channel_img -= channel_img.mean()\n                channel_img /= channel_img.std()\n                channel_img *= 64\n                channel_img += 128\n                channel_img = np.clip(channel_img, 0, 255).astype('uint8')\n            \n                # Adding the current channel to the grid\n                grid[i * size:(i + 1) * size, j * size:(j + 1) * size] = channel_img\n        \n        # Scaling the current grid\n        scale = 1 / size\n        plt.figure(figsize=(scale*grid.shape[1], scale*grid.shape[0]))\n        plt.imshow(grid, aspect='auto')\n        plt.title(layer_name)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T11:02:50.000548Z","iopub.execute_input":"2022-01-01T11:02:50.000803Z","iopub.status.idle":"2022-01-01T11:02:50.017584Z","shell.execute_reply.started":"2022-01-01T11:02:50.000774Z","shell.execute_reply":"2022-01-01T11:02:50.016669Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Ploting the first 12 layers of the VGG19 Model\nplot_interm_act(model_vgg, 12, p, DIMS_VGG, \"VGG19\")","metadata":{"execution":{"iopub.status.busy":"2022-01-01T11:02:57.343765Z","iopub.execute_input":"2022-01-01T11:02:57.344439Z","iopub.status.idle":"2022-01-01T11:03:06.875208Z","shell.execute_reply.started":"2022-01-01T11:02:57.344392Z","shell.execute_reply":"2022-01-01T11:03:06.874236Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"plot_interm_act(model_conv_2d, 10, p, (HEIGHT, WIDTH, 3), \"CONV2D\")","metadata":{"execution":{"iopub.status.busy":"2022-01-01T11:11:36.573652Z","iopub.execute_input":"2022-01-01T11:11:36.573904Z","iopub.status.idle":"2022-01-01T11:11:42.475657Z","shell.execute_reply.started":"2022-01-01T11:11:36.573874Z","shell.execute_reply":"2022-01-01T11:11:42.475033Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}